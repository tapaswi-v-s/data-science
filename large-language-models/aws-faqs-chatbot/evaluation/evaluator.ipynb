{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "1. [Instructions](#instructions)\n",
    "2. [Introduction on Evaluation](#introduction)\n",
    "3. [Library Imports](#library_imports)\n",
    "4. [FAQ Preparation](#faq_preparation)\n",
    "5. [Answer Generation](#answer_generation)\n",
    "6. [ChatBot Response Evaluation](#chatbot_evaluation)\n",
    "    - [Latency](#latency) \n",
    "    - [Answer Similarity](#answer_similarity)\n",
    "    - [Faithfullness](#faithfullness)\n",
    "    - [Relevance](#relevance)\n",
    "    - [Answer Correctness](#answer_correctness)\n",
    "7. [Context Evaluation](#context_evaluation)\n",
    "    - [Entity Recall](#entity_recall)\n",
    "8. [Proposed Methods of Improvement](#methods_of_improvement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Instruction before running this notebook\n",
    "<a id='instructions'></a>\n",
    "\n",
    "- Paste your huggingface api token in the [.env](./.env) file\n",
    "- Install the [requirements](./requirements.txt) using `pip install -r requirements.txt`\n",
    "- Run the command `python -m spacy download en`. This will download the default English language model for the SpaCy library for extracting the entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id='introduction'></a>\n",
    "\n",
    "For evaluating the responses of our ChatBot, I have established an evaluation mechanism centered around four key metrics:\n",
    "- **Answer Similarity**\n",
    "- **Faithfulness**\n",
    "- **Relevance**\n",
    "- **Answer Correctness**\n",
    "\n",
    "**How this works:**\n",
    "- I have created an LLM Evaluator for each metric, incorporating specific evaluation criteria and a scoring rubric ranging from **1 to 5** (where **1** denotes the lowest score and **5** denotes the highest score).\n",
    "- This criteria and scoring rubric are provided to an Evaluator LLM along with the generated response and the retrieved context from which the response is generated. After processing the input, the Evaluator LLM outputs:\n",
    "\n",
    "    1. A numeric score between **1 and 5**\n",
    "    2. Feedback explaining why this particular score was given\n",
    "\n",
    "Each metric and its scoring criteria are defined in detail later in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "<a id='library_imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import warnings, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Question to be asked to ChatBot\n",
    "<a id='faq_preparation'></a>\n",
    "Here I'm extracting few question from the data to be asked to the ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the questions to be asked to chatbot\n",
    "aws_faq = pd.read_csv('../aws_faqs.csv')\n",
    "X_train, X_test = train_test_split(aws_faq, test_size=0.02, random_state=42)\n",
    "aws_faq = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answers using Chatbot\n",
    "<a id='answer_generation'></a>\n",
    "Methodology:\n",
    "- Send each questions from the data frame to ChatBot.\n",
    "- ChatBot will generate the answers and return the generated answer.\n",
    "- Create a data frame named ```chatbot_response_df``` containing the generated answers and its relevant docs from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\tapas\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "WARNING:tensorflow:From c:\\Users\\tapas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the chatpot\n",
    "from aws_faq_chatbot import AwsFaqChatBot\n",
    "chatbot = AwsFaqChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:37,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generating answers and store it in the dataframe\n",
    "chatbot_output = []\n",
    "for row in tqdm(aws_faq.iterrows()):\n",
    "    start_time = time.time()\n",
    "    answer, relevant_docs = chatbot.ask(row[1]['question'])\n",
    "    end_time = time.time()\n",
    "\n",
    "    chatbot_output.append({\n",
    "        'question': row[1]['question'],\n",
    "        'generated_answer': answer, \n",
    "        'context': relevant_docs,\n",
    "        'latency(seconds)': round((end_time - start_time), 2)})\n",
    "\n",
    "chatbot_response_df = pd.DataFrame(chatbot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot Response Evaluation\n",
    "<a id='chatbot_evaluation'></a>\n",
    "\n",
    "Following are the 5 metrics for evaluating the chatbor response:\n",
    "- Latency\n",
    "- Answer Similarity\n",
    "- Fithfullness\n",
    "- Relevance\n",
    "- Answer Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latency <a id='latency'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Q. Can I tag a Spot Fleet request?\n",
      "Answer:  Yes, you can tag a Spot Fleet request when you create it. However, the Spot Fleet itself cannot be tagged.\n",
      "Context: ['You can request to launch Spot Instances with tags via Spot Fleet. The Fleet by itself cannot be tagged.']\n",
      "Latency(seconds): 3.18\n",
      "===================================================\n",
      "\n",
      "Question: Q: Where can I learn more about EFS?\n",
      "Answer: \n",
      "    You can learn more about Amazon EFS by visiting the Amazon EFS FAQ page.\n",
      "Context: ['You can visit the Amazon EFS FAQ pageAmazon EFS FAQ page.']\n",
      "Latency(seconds): 1.43\n",
      "===================================================\n",
      "\n",
      "Question: Q: Do RIs provide a capacity reservation?\n",
      "Answer:  A: Yes, RIs do provide a capacity reservation. As per the context, to take advantage of the capacity reservation, one should buy a Reserved Instance (RI) in a specific Availability Zone (AZ).\n",
      "Context: ['If you want to take advantage of the capacity reservation, then you should buy an RI in a specific AZ.']\n",
      "Latency(seconds): 3.89\n",
      "===================================================\n",
      "\n",
      "Question: Q: What happens to my data when a system terminates?\n",
      "Answer: \n",
      "    When a system terminates, the root device and attached device data are stored on the corresponding EBS volumes. However, memory (RAM) contents are not saved. Therefore, any data stored in the memory will be lost.\n",
      "Context: ['As with the Stop feature, root device and attached device data are stored on the corresponding EBS volumes. Memory (RAM) contents are stored on the EBS root volume.']\n",
      "Latency(seconds): 2.35\n",
      "===================================================\n",
      "\n",
      "Question: Q: Can I still use/add more Previous Generation instances?\n",
      "Answer:  Yes, you can still use and add more Previous Generation instances. They are not going away.\n",
      "Context: ['No. Your Reserved Instances will not change, and the Previous Generation instances are not going away.']\n",
      "Latency(seconds): 1.03\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restrincting to only 5 output for the sake of readability\n",
    "for row in chatbot_response_df.head().iterrows():\n",
    "    print(f\"Question: {row[1]['question']}\")\n",
    "    print(f\"Answer: {row[1]['generated_answer']}\")\n",
    "    print(f\"Context: {row[1]['context']}\")\n",
    "    print(f\"Latency(seconds): {row[1]['latency(seconds)']}\")\n",
    "    print('===================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Latency of the ChatBot: 2.92 Seconds\n"
     ]
    }
   ],
   "source": [
    "# calculating the overall latency of the ChatBot\n",
    "chatbot_response_df['latency(seconds)'] = chatbot_response_df['latency(seconds)'].astype(float)\n",
    "\n",
    "print(\"Overall Latency of the ChatBot:\"\n",
    "f\" {round(np.mean(chatbot_response_df['latency(seconds)']), 2)} Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving generated response .csv file\n",
    "chatbot_response_df.to_csv('./scores/chatbot_response.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Similarity \n",
    "<a id='answer_similarity'></a>\n",
    "Answer Similarity answers the question: __Is the response semantically similar based on the context?__\n",
    "\n",
    "Answer similarity is scored based on how closely the output's meaning matches the target, with higher scores for greater alignment.\n",
    "\n",
    "__Scoring Criteria__\n",
    "- _Score __1__:_ The response has little to no semantic similarity to the reference answer.\n",
    "- _Score __2__:_ The response displays partial semantic similarity to the reference answer on some aspects.\n",
    "- _Score __3__:_ The response has moderate semantic similarity to the reference answer.\n",
    "- _Score __4__:_ The response aligns with the reference answer in most aspects and has substantial semantic similarity.\n",
    "- _Score __5__:_ The response closely aligns with the reference answer in all significant aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\tapas\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:50,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "from metrics.answer_similarity import AnswerSimilarity\n",
    "\n",
    "ans_similarity = AnswerSimilarity()\n",
    "answer_similarity_scores = ans_similarity.evaluate(chatbot_response_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Score result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Q. Can I tag a Spot Fleet request?\n",
      "Answer:  Yes, you can tag a Spot Fleet request when you create it. However, the Spot Fleet itself cannot be tagged.\n",
      "Context: ['You can request to launch Spot Instances with tags via Spot Fleet. The Fleet by itself cannot be tagged.']\n",
      "Similarity Score: 5\n",
      "Feedback: The response correctly identifies that Spot Fleet requests can be tagged during creation, and also clarifies that the Spot Fleet itself cannot be tagged. This is semantically similar to the reference answer, which states that Spot Instances can be launched with tags via Spot Fleet, and that the Fleet itself cannot be tagged.\n",
      "===================================================\n",
      "\n",
      "Question: Q: Where can I learn more about EFS?\n",
      "Answer: \n",
      "    You can learn more about Amazon EFS by visiting the Amazon EFS FAQ page.\n",
      "Context: ['You can visit the Amazon EFS FAQ pageAmazon EFS FAQ page.']\n",
      "Similarity Score: 5\n",
      "Feedback: The provided response, 'You can learn more about Amazon EFS by visiting the Amazon EFS FAQ page.', has a high degree of semantic similarity to the reference answer, 'You can visit the Amazon EFS FAQ pageAmazon EFS FAQ page.'. Both provide the same information and direct the user to the Amazon EFS FAQ page.\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restrincting to only 2 output for the sake of readability\n",
    "for row in answer_similarity_scores.head(2).iterrows():\n",
    "    print(f\"Question: {row[1]['question']}\")\n",
    "    print(f\"Answer: {row[1]['generated_answer']}\")\n",
    "    print(f\"Context: {row[1]['context']}\")\n",
    "    print(f\"Similarity Score: {row[1]['similarity_score']}\")\n",
    "    print(f\"{row[1]['similarity_score_feedback']}\")\n",
    "    print('===================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Similarity Score Rating of the ChatBot: 4.08\n"
     ]
    }
   ],
   "source": [
    "# calculating the overall Similarity score of the ChatBot\n",
    "answer_similarity_scores['similarity_score'] = answer_similarity_scores['similarity_score'].astype(int)\n",
    "\n",
    "print(\"Overall Similarity Score Rating of the ChatBot:\"\n",
    "f\" {round(np.mean(answer_similarity_scores['similarity_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to similarity scores .csv file\n",
    "answer_similarity_scores.to_csv('./scores/answer_similarity_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfullness\n",
    "<a id='faithfullness'></a>\n",
    "Faithfullness answers the question: __Does the response accurately reflect the given context?__\n",
    "\n",
    "Faithfulness checks if the response is factually consistent with the context.\n",
    "\n",
    "__Scoring Criteria__\n",
    "- _Score __1__:_ None of the claims in the response can be inferred from the provided context.\n",
    "- _Score __2__:_ Some of the claims in the response can be inferred from the provided context, but the majority of the response is missing from, inconsistent with, or contradictory to the provided context.\n",
    "- _Score __3__:_ Half or more of the claims in the response can be inferred from the provided context.\n",
    "- _Score __4__:_ Most of the claims in the response can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n",
    "- _Score __5__:_ All of the claims in the response are directly supported by the provided context, demonstrating high faithfulness to the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\tapas\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:01,  4.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from metrics.faithfullness import Faithfullness\n",
    "\n",
    "faithfullness = Faithfullness()\n",
    "faithfullness_scores = faithfullness.evaluate(chatbot_response_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfullness result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Q. Can I tag a Spot Fleet request?\n",
      "Answer:  Yes, you can tag a Spot Fleet request when you create it. However, the Spot Fleet itself cannot be tagged.\n",
      "Context: ['You can request to launch Spot Instances with tags via Spot Fleet. The Fleet by itself cannot be tagged.']\n",
      "Faithfullness Score: 5\n",
      "Feedback: The response correctly identifies that Spot Fleet requests can be tagged when created, and correctly identifies that the Spot Fleet itself cannot be tagged. These claims are directly supported by the context.\n",
      "===================================================\n",
      "\n",
      "Question: Q: Where can I learn more about EFS?\n",
      "Answer: \n",
      "    You can learn more about Amazon EFS by visiting the Amazon EFS FAQ page.\n",
      "Context: ['You can visit the Amazon EFS FAQ pageAmazon EFS FAQ page.']\n",
      "Faithfullness Score: 5\n",
      "Feedback: The provided response accurately directs the user to learn more about Amazon EFS by visiting the Amazon EFS FAQ page, which is directly supported by the provided context.\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restrincting to only 2 output for the sake of readability\n",
    "for row in faithfullness_scores.head(2).iterrows():\n",
    "    print(f\"Question: {row[1]['question']}\")\n",
    "    print(f\"Answer: {row[1]['generated_answer']}\")\n",
    "    print(f\"Context: {row[1]['context']}\")\n",
    "    print(f\"Faithfullness Score: {row[1]['faithfullness_score']}\")\n",
    "    print(f\"{row[1]['faithfullness_score_feedback']}\")\n",
    "    print('===================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Faithfullness Score Rating of the ChatBot: 4.31\n"
     ]
    }
   ],
   "source": [
    "# calculating the overall Faithfullness Score of the ChatBot\n",
    "faithfullness_scores['faithfullness_score'] = faithfullness_scores['faithfullness_score'].astype(int)\n",
    "\n",
    "print(\"Overall Faithfullness Score Rating of the ChatBot:\"\n",
    "f\" {round(np.mean(faithfullness_scores['faithfullness_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to faithfullness scores .csv file\n",
    "faithfullness_scores.to_csv('./scores/faithfullness_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance\n",
    "<a id='relevance'></a>\n",
    "Relevance answers the question: __Does the response address the given question and context?__\n",
    "\n",
    "Relevance measures how well the response addresses the question and context. Scores reflect how directly the output answers the input question within the context.\n",
    "\n",
    "__Scoring Criteria__\n",
    "- _Score __1__:_ The response doesn't mention anything about the question or is completely irrelevant to the provided context.\n",
    "- _Score __2__:_ The response provides some relevance to the question and is somehow related to the provided context.\n",
    "- _Score __3__:_ The response mostly answers the question and is largely consistent with the provided context.\n",
    "- _Score __4__:_ The response answers the question and is consistent with the provided context.\n",
    "- _Score __5__:_ The response answers the question comprehensively using the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\tapas\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:12,  5.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from metrics.relevance import Relevance\n",
    "\n",
    "relevance = Relevance()\n",
    "relevance_scores = relevance.evaluate(chatbot_response_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevance result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Q. Can I tag a Spot Fleet request?\n",
      "Answer:  Yes, you can tag a Spot Fleet request when you create it. However, the Spot Fleet itself cannot be tagged.\n",
      "Context: ['You can request to launch Spot Instances with tags via Spot Fleet. The Fleet by itself cannot be tagged.']\n",
      "Relevance Score: 5\n",
      "Feedback: The response directly addresses the question and is consistent with the provided context. The context states that Spot Fleet cannot be tagged, and the response confirms this by stating that you can tag a Spot Fleet request when you create it, but the Spot Fleet itself cannot be tagged.\n",
      "===================================================\n",
      "\n",
      "Question: Q: Where can I learn more about EFS?\n",
      "Answer: \n",
      "    You can learn more about Amazon EFS by visiting the Amazon EFS FAQ page.\n",
      "Context: ['You can visit the Amazon EFS FAQ pageAmazon EFS FAQ page.']\n",
      "Relevance Score: 4\n",
      "Feedback: The response directly addresses the question by suggesting a place to learn more about EFS. The context supports this by explicitly mentioning the Amazon EFS FAQ page. However, the response could have been more comprehensive by explaining what the FAQ page is or providing additional resources.\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restrincting to only 2 output for the sake of readability\n",
    "for row in relevance_scores.head(2).iterrows():\n",
    "    print(f\"Question: {row[1]['question']}\")\n",
    "    print(f\"Answer: {row[1]['generated_answer']}\")\n",
    "    print(f\"Context: {row[1]['context']}\")\n",
    "    print(f\"Relevance Score: {row[1]['relevance_score']}\")\n",
    "    print(f\"{row[1]['relevance_score_feedback']}\")\n",
    "    print('===================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Relevance Scores Rating of the ChatBot: 4.54\n"
     ]
    }
   ],
   "source": [
    "# calculating the overall Relevance Scores of the ChatBot\n",
    "relevance_scores['relevance_score'] = relevance_scores['relevance_score'].astype(int)\n",
    "\n",
    "print(\"Overall Relevance Scores Rating of the ChatBot:\"\n",
    "f\" {round(np.mean(relevance_scores['relevance_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to relevance scores .csv file\n",
    "relevance_scores.to_csv('./scores/relevance_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Correctness\n",
    "<a id='answer_correctness'></a>\n",
    "Answer Correctness answers the question: __Is the response pertinent to the question and context?__\n",
    "\n",
    "Answer correctness is evaluated based on how accurately the response matches the target. Scores reflect the degree of semantic similarity and factual correctness, with higher scores indicating greater accuracy.\n",
    "\n",
    "__Scoring Criteria__\n",
    "- _Score __1__:_ The response is completely incorrect. It is completely different from or contradicts the provided target.\n",
    "- _Score __2__:_ The response demonstrates some degree of semantic similarity and includes partially correct information. However, the response still has significant discrepancies with the provided target or inaccuracies.\"\n",
    "- _Score __3__:_ The response addresses a couple of aspects of the input accurately, aligning with the provided target. However, there are still omissions or minor inaccuracies.\n",
    "- _Score __4__:_ The response is mostly correct. It provides mostly accurate information, but there may be one or more minor omissions or inaccuracies.\n",
    "- _Score __5__:_ The response is correct. It demonstrates a high degree of accuracy and semantic similarity to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\tapas\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [02:00,  9.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from metrics.answer_correctness import AnswerCorrectness\n",
    "\n",
    "answer_correctness = AnswerCorrectness()\n",
    "answer_correctness_scores = answer_correctness.evaluate(chatbot_response_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Correctness result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Q. Can I tag a Spot Fleet request?\n",
      "Answer:  Yes, you can tag a Spot Fleet request when you create it. However, the Spot Fleet itself cannot be tagged.\n",
      "Context: ['You can request to launch Spot Instances with tags via Spot Fleet. The Fleet by itself cannot be tagged.']\n",
      "Correctness Score: 4\n",
      "Feedback: The response correctly explains that a Spot Fleet request can be tagged during creation, and clarifies that the Spot Fleet itself cannot be tagged, which is in line with the target. However, the response could have been more detailed in explaining the concept of tagging Spot Fleet requests.\n",
      "===================================================\n",
      "\n",
      "Question: Q: Where can I learn more about EFS?\n",
      "Answer: \n",
      "    You can learn more about Amazon EFS by visiting the Amazon EFS FAQ page.\n",
      "Context: ['You can visit the Amazon EFS FAQ pageAmazon EFS FAQ page.']\n",
      "Correctness Score: 4\n",
      "Feedback: The response is relevant to the given question and it includes the correct information. It suggests visiting the Amazon EFS FAQ page which aligns with the provided target. However, it doesn't directly address the 'learn more' part of the question.\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restrincting to only 2 output for the sake of readability\n",
    "for row in answer_correctness_scores.head(2).iterrows():\n",
    "    print(f\"Question: {row[1]['question']}\")\n",
    "    print(f\"Answer: {row[1]['generated_answer']}\")\n",
    "    print(f\"Context: {row[1]['context']}\")\n",
    "    print(f\"Correctness Score: {row[1]['correctness_score']}\")\n",
    "    print(f\"{row[1]['correctness_score_feedback']}\")\n",
    "    print('===================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Correctness Score Rating of the ChatBot: 3.85\n"
     ]
    }
   ],
   "source": [
    "# calculating the overall Faithfullness Score of the ChatBot\n",
    "answer_correctness_scores['correctness_score'] = answer_correctness_scores['correctness_score'].astype(int)\n",
    "\n",
    "print(\"Overall Correctness Score Rating of the ChatBot:\"\n",
    "f\" {round(np.mean(answer_correctness_scores['correctness_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to answer correctness scores .csv file\n",
    "answer_correctness_scores.to_csv('./scores/answer_correctness_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Evaluation\n",
    "<a id='context_evaluation'></a>\n",
    "\n",
    "To ensure the accuracy and relevance of the retrieved contexts in the ChatBot responses, I have use the **Entity Recall** metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recall\n",
    "<a id='entity_recall'></a>\n",
    "\n",
    "**Entity Recall** measures the system's ability to correctly recall all relevant entities within the context compared to a set of reference entities. This metric evaluates whether the system can identify and retrieve all key entities necessary for a comprehensive understanding of the query context.\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "- **Extract Entities**: Use Named Entity Recognition (NER) tools such as **SpaCy** and **NLTK** to dynamically extract entities from both the retrieved context and the generated answer.\n",
    "- **Compare Entities**: Compare the extracted entities from the retrieved context with those from the generated answer.\n",
    "- **Calculate Entity Recall**: Calculate the recall as the number of correctly recalled entities divided by the total number of relevant entities in the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents]\n",
    "\n",
    "def entity_recall(true_entities, retrieved_entities):\n",
    "    true_positive = len(set(true_entities) & set(retrieved_entities))\n",
    "    recall = true_positive / len(true_entities) if true_entities else 0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:01, 12.27it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_recall_output = []\n",
    "for row in tqdm(chatbot_response_df.iterrows()):    \n",
    "    generated_answer_entities = extract_entities(row[1]['generated_answer'])\n",
    "    context_entities = extract_entities(row[1]['context'][0])\n",
    "\n",
    "    entity_recall_output.append(entity_recall(context_entities, generated_answer_entities))\n",
    "\n",
    "entity_recall_scores = pd.concat([pd.DataFrame({'entity_recall_score': entity_recall_output}), chatbot_response_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Recall result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Q. Can I tag a Spot Fleet request?\n",
      "Answer:  Yes, you can tag a Spot Fleet request when you create it. However, the Spot Fleet itself cannot be tagged.\n",
      "Context: ['You can request to launch Spot Instances with tags via Spot Fleet. The Fleet by itself cannot be tagged.']\n",
      "Entity Recall Score: 0.5\n",
      "===================================================\n",
      "\n",
      "Question: Q: Where can I learn more about EFS?\n",
      "Answer: \n",
      "    You can learn more about Amazon EFS by visiting the Amazon EFS FAQ page.\n",
      "Context: ['You can visit the Amazon EFS FAQ pageAmazon EFS FAQ page.']\n",
      "Entity Recall Score: 0.67\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restrincting to only 2 output for the sake of readability\n",
    "for row in entity_recall_scores.head(2).iterrows():\n",
    "    print(f\"Question: {row[1]['question']}\")\n",
    "    print(f\"Answer: {row[1]['generated_answer']}\")\n",
    "    print(f\"Context: {row[1]['context']}\")\n",
    "    print(f\"Entity Recall Score: {round(row[1]['entity_recall_score'], 2)}\")\n",
    "    print('===================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Entity Recall Score of the ChatBot: 0.08\n"
     ]
    }
   ],
   "source": [
    "# calculating the overall Faithfullness Score of the ChatBot\n",
    "entity_recall_scores['entity_recall_score'] = entity_recall_scores['entity_recall_score'].astype(int)\n",
    "\n",
    "print(\"Overall Entity Recall Score of the ChatBot:\"\n",
    "f\" {round(np.mean(entity_recall_scores['entity_recall_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to answer correctness scores .csv file\n",
    "entity_recall_scores.to_csv('./scores/entity_recall_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Improving ChatBot response <a id='methods_of_improvement'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the ChatBot performs well and provides relevant responses based on the FAQs. However, several improvements can be made to enhance its response quality. These improvements are as follows:\n",
    "\n",
    "1. **Improved Chunking and Text Splitting**\n",
    "    - _Current Approach_: The entire answer is converted to embeddings and stored in the vector store.\n",
    "    - _Improvement_: Utilize more sophisticated chunking and text splitting techniques to ensure meaningful segments are created for better embedding and retrieval.\n",
    "2. **Use Different Vector Stores for Context Retrieval**\n",
    "    - _Current Approach_: The ChatBot currently uses Chroma DB.\n",
    "    - _Improvement_: Experiment with different vector stores such as Pinecone, Weaviate, FAISS, etc., to optimize context retrieval and potentially improve performance.\n",
    "3. **User Feedback on the Response**\n",
    "    - _Current Approach_: No feedback mechanism is in place.\n",
    "    - _Improvement_: Integrate a feedback module to record users' feedback on the generated responses, allowing for continuous learning and improvement based on real user interactions.\n",
    "4. **Using Different LLMs for Better Response Generation and Evaluation Metrics**\n",
    "    - _Current Approach_: The ChatBot uses Mixtral-8x7B-Instruct-v0.1.\n",
    "    - _Improvement_: Experiment with different models such as Meta's LLAMA, Google's Gemma, and OpenAI's models to enhance response generation and evaluation metrics.\n",
    "5. **Improved Irrelevant Query Handling**\n",
    "    - _Current Approach_: There is no specific mechanism for handling out-of-context queries.\n",
    "    - _Improvement_: Implement a separate LLM specifically for filtering out and processing out-of-context queries to ensure the ChatBot handles irrelevant queries more effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
